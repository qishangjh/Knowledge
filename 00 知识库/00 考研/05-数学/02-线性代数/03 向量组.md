---
科目:
课程名称:
tags: []
一轮复习情况: 进行中
二轮复习情况: 未开始
三轮复习情况: 未开始
难度:
考频:
备注:
---

# 03 向量组
## 03-1 向量与向量组的线性相关性
### 1. 向量的定义
- **n维向量** ：n个数过程一个有序数组$[a_1, a_2, \ldots, a_n]$，称为一个n维向量
- **列向量**：一个 $n \times 1$ 的矩阵，表示为 $\mathbf{v} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}$。
- **行向量**：一个 $1 \times n$ 的矩阵，表示为 $\mathbf{v}^T = (v_1, v_2, \ldots, v_n)$。
向量可以看作是空间中的一个点或者一个有方向的线段，它具有大小和方向。

**相等**：若$a =[a_1，a_2,…,a_n]$，$\beta=[b_1,b_2,…b_n]$都是n维向量，则当且仅当 $a_i=b_i$时，$\alpha=\beta$ .（ 对应元素都相等） .
**加法** ：$\alpha +\beta = [a_1 +b_1 ,a_2 +b_2 ，…，a_n +b_n]$（ a同上 ）.（ 对应元素相加）
**数乘**： $ka= [ka_1,ka_2,…，ka_3]$（ a同上，k为实数 ）.（ 用常数乘向量中每一个元素）
### 2. 向量的内积与正交
- **内积（点积）**：对于两个 $n$ 维向量 $\mathbf{u} = (u_1, \ldots, u_n)$ 和 $\mathbf{v} = (v_1, \ldots, v_n)$，它们的内积定义为：
    $$ \mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n $$
    内积的结果是一个标量。
- **正交**：如果两个非零向量的内积为零，即 $\mathbf{u} \cdot \mathbf{v} = 0$，则称这两个向量**正交**。在几何上，这意味着它们互相垂直。

### 3. 正交矩阵
- **定义**：一个方阵 $A$ 如果满足 $A^T A = E$（或等价地，$A A^T = E$，或 $A^{-1} = A^T$），则称其为**正交矩阵**。
- **性质**：
    - 正交矩阵的列向量（或行向量）组是**标准正交基**（即两两正交且模长为1）。
    - 正交变换（由正交矩阵表示的线性变换）保持向量的长度和夹角，是刚体变换（旋转或反射）。
    - 行列式为 $\pm 1$。
- **重要性**：在实对称矩阵的对角化、二次型的化简中扮演关键角色。

### 4. 线性组合
给定一个向量组 $S = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m\}$ 和一组实数（标量）$k_1, k_2, \ldots, k_m$，表达式：
$$ k_1 \mathbf{v}_1 + k_2 \mathbf{v}_2 + \cdots + k_m \mathbf{v}_m $$
称为向量组 $S$ 的一个**线性组合**。

### 5. 线性表示
- **能表示**：如果向量 $\mathbf{b}$ 可以写成向量组 $S = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m\}$ 的一个线性组合，即存在标量 $k_1, \ldots, k_m$ 使得 $\mathbf{b} = k_1 \mathbf{v}_1 + \cdots + k_m \mathbf{v}_m$，则称向量 $\mathbf{b}$ 可以被向量组 $S$ **线性表示**。
- **不能表示**：如果不存在这样的标量，则不能线性表示。
- **判断方法**：将问题转化为线性方程组 $k_1 \mathbf{v}_1 + \cdots + k_m \mathbf{v}_m = \mathbf{b}$，即 $A\mathbf{k} = \mathbf{b}$，其中 $A = (\mathbf{v}_1, \ldots, \mathbf{v}_m)$。如果方程组有解，则能表示；无解，则不能表示。这等价于判断 $rank(A) = rank(A|\mathbf{b})$ 是否成立。

### 6. 线性相关性
- **相关**：给定一个向量组 $S = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m\}$，如果存在**不全为零**的标量 $k_1, k_2, \ldots, k_m$，使得：
    $$ k_1 \mathbf{v}_1 + k_2 \mathbf{v}_2 + \cdots + k_m \mathbf{v}_m = \mathbf{0} $$
    则称向量组 $S$ **线性相关**。
    - 直观理解：至少有一个向量可以被其他向量线性表示。
- **无关**：如果只有当所有标量 $k_1, k_2, \ldots, k_m$ 都为零时，上述等式才成立，则称向量组 $S$ **线性无关**。
    - 直观理解：每个向量都是"独立"的，不能被其他向量表示。

### 7. 判别的七大定理（或重要结论）
这里我们总结一些判断向量组线性相关性的重要定理和方法：
1. **定义法**：直接根据定义，看齐次线性方程组 $k_1 \mathbf{v}_1 + \cdots + k_m \mathbf{v}_m = \mathbf{0}$ 是否有非零解。
    - 将向量组写成矩阵 $A = (\mathbf{v}_1, \ldots, \mathbf{v}_m)$，则问题转化为 $A\mathbf{k} = \mathbf{0}$ 是否有非零解。
2. **秩判别法**：
    - 将向量组 $\{\mathbf{v}_1, \ldots, \mathbf{v}_m\}$ 构成一个矩阵 $A = (\mathbf{v}_1, \ldots, \mathbf{v}_m)$（列向量）或 $A = (\mathbf{v}_1^T, \ldots, \mathbf{v}_m^T)^T$（行向量）。
    - 如果 $rank(A) = m$（向量的个数），则向量组线性无关。
    - 如果 $rank(A) < m$，则向量组线性相关。
3. **行列式判别法（仅限方阵）**：
    - 如果向量组的个数 $m$ 等于向量的维数 $n$，即构成一个方阵 $A$。
    - 如果 $\det(A) \neq 0$，则向量组线性无关。
    - 如果 $\det(A) = 0$，则向量组线性相关。

4. **部分组与整体组的关系**：
    - 如果向量组的一部分线性相关，则整个向量组也线性相关。
    - 如果整个向量组线性无关，则它的任何一部分也线性无关。

5. **零向量的影响**：
    - 如果向量组中包含零向量，则该向量组一定线性相关。
    - 单个非零向量是线性无关的。

6. **向量个数与维数的关系**：
    - 如果 $m > n$（向量的个数大于向量的维数），则该向量组一定线性相关。
    - 例如，在三维空间中，任意四个或更多向量一定是线性相关的。

7. **等价向量组的秩相等**：
    - 如果向量组 $\alpha_1, \ldots, \alpha_m$ 可以被向量组 $\beta_1, \ldots, \beta_s$ 线性表示，且 $\beta_1, \ldots, \beta_s$ 也可以被 $\alpha_1, \ldots, \alpha_m$ 线性表示，则称这两个向量组等价。
    - 等价向量组的秩相等。

## 03-2 极大线性无关组与向量的秩

### 1. 极大线性无关组 (Maximal Linearly Independent Subset)
- **定义**：给定一个向量组 $S = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m\}$。如果它的一个子集 $S' = \{\mathbf{v}_{i_1}, \mathbf{v}_{i_2}, \ldots, \mathbf{v}_{i_r}\}$ 满足以下两个条件：
    1. $S'$ 是**线性无关**的。
    2. $S'$ 是**极大**的，即在 $S'$ 中再添加 $S$ 中任何一个不在 $S'$ 中的向量，所得到的新的向量组都将是**线性相关**的。
    那么，这个子集 $S'$ 就称为向量组 $S$ 的一个**极大线性无关组**。

- **直观理解**：
    - 它是一组"最精简"的、相互独立的向量。
    - 它包含了原向量组中所有"本质上不同"的方向。
    - 原向量组中的任何一个向量，都可以被这个极大线性无关组线性表示。
- **重要性质**：
    - 一个向量组的极大线性无关组**不唯一**，但它们所含向量的个数是唯一的。
    - 任何一个向量组的极大线性无关组都可以作为原向量组的**基底**。
- **如何求？**
    1. 将向量组 $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m\}$ 作为列向量（或行向量）构成一个矩阵 $A$。
    2. 对矩阵 $A$ 进行初等行变换，将其化为行阶梯形矩阵。
    3. 行阶梯形矩阵中**主元（非零首元）所在的列**对应的原矩阵 $A$ 中的列向量，就构成一个极大线性无关组。
    4. （如果作为行向量，则是主元所在的行对应的原矩阵 $A$ 中的行向量）。

    **示例**：求向量组 $\{\mathbf{v}_1=(1,2,1)^T, \mathbf{v}_2=(2,4,2)^T, \mathbf{v}_3=(3,6,3)^T, \mathbf{v}_4=(1,1,1)^T\}$ 的一个极大线性无关组。
    构成矩阵 $A = (\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3, \mathbf{v}_4)$：
    $$ A = \begin{pmatrix} 1 & 2 & 3 & 1 \\ 2 & 4 & 6 & 1 \\ 1 & 2 & 3 & 1 \end{pmatrix} $$
    进行初等行变换：
    $$ \begin{pmatrix} 1 & 2 & 3 & 1 \\ 2 & 4 & 6 & 1 \\ 1 & 2 & 3 & 1 \end{pmatrix} \xrightarrow{R_2 - 2R_1} \begin{pmatrix} 1 & 2 & 3 & 1 \\ 0 & 0 & 0 & -1 \\ 1 & 2 & 3 & 1 \end{pmatrix} $$
    $$ \xrightarrow{R_3 - R_1} \begin{pmatrix} 1 & 2 & 3 & 1 \\ 0 & 0 & 0 & -1 \\ 0 & 0 & 0 & 0 \end{pmatrix} $$
    $$ \xrightarrow{R_2 \times (-1)} \begin{pmatrix} 1 & 2 & 3 & 1 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 \end{pmatrix} $$
    主元在第 1 列和第 4 列。
    所以，原向量组的第 1 个向量 $\mathbf{v}_1$ 和第 4 个向量 $\mathbf{v}_4$ 构成一个极大线性无关组：$\{\mathbf{v}_1, \mathbf{v}_4\}$。

### 2. 向量组的秩 (Rank of a Vector Set)

- **定义**：向量组 $S = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_m\}$ 的秩，就是其**极大线性无关组中所含向量的个数**。
- **表示**：通常记作 $rank(S)$ 或 $r(S)$。
- **与矩阵秩的关系**：
    - 如果将向量组作为列向量构成矩阵 $A = (\mathbf{v}_1, \ldots, \mathbf{v}_m)$，那么向量组的秩就等于矩阵 $A$ 的列秩，也等于矩阵 $A$ 的行秩，最终等于**矩阵 $A$ 的秩**。
    - 所以，计算向量组的秩，就是计算由这些向量构成的矩阵的秩。
- **重要性**：
    - 向量组的秩反映了向量组的"本质维数"或者说"自由度"。
    - 它告诉我们，在原向量组中，有多少个向量是真正独立的，不能被其他向量表示的。
    - 在解线性方程组时，齐次方程组的解空间的维数就是 $n - rank(A)$，这里的 $rank(A)$ 就是系数矩阵的列向量组的秩。
- **计算方法**：
    1. 将向量组构成一个矩阵 $A$。
    2. 对矩阵 $A$ 进行初等行变换，将其化为行阶梯形矩阵。
    3. 行阶梯形矩阵中**非零行的行数**就是矩阵 $A$ 的秩，也即向量组的秩。

    **示例**：计算上面向量组 $\{\mathbf{v}_1=(1,2,1)^T, \mathbf{v}_2=(2,4,2)^T, \mathbf{v}_3=(3,6,3)^T, \mathbf{v}_4=(1,1,1)^T\}$ 的秩。
    我们已经将其化为行阶梯形矩阵：
    $$ \begin{pmatrix} 1 & 2 & 3 & 1 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 \end{pmatrix} $$
    这个行阶梯形矩阵有 2 个非零行。
    所以，向量组的秩为 2。这也与我们求得的极大线性无关组中包含 2 个向量相符。

理解了极大线性无关组和向量组的秩，你就掌握了线性代数中"基底"和"维数"的核心思想。这对于后续理解向量空间、线性映射等概念都至关重要。继续加油！
## 03-3 等价向量组
好的，咱们继续向量组的旅程，这次要聊的是**等价向量组**。这个概念能帮助我们理解不同向量组之间的一种"等价关系"，它们虽然可能看起来不一样，但在线性表示能力上却是相同的。

### 1. 定义

给定两个向量组 $S_1 = \{\mathbf{\alpha}_1, \mathbf{\alpha}_2, \ldots, \mathbf{\alpha}_m\}$ 和 $S_2 = \{\mathbf{\beta}_1, \mathbf{\beta}_2, \ldots, \mathbf{\beta}_s\}$。

如果向量组 $S_1$ 中的**每一个向量**都可以被向量组 $S_2$ **线性表示**，并且向量组 $S_2$ 中的**每一个向量**也都可以被向量组 $S_1$ **线性表示**，那么我们就称向量组 $S_1$ 与向量组 $S_2$ **等价**。

记作 $S_1 \simeq S_2$。

**直观理解**：
等价向量组就像是两个不同的"工具箱"，虽然里面的工具可能不一样，但它们能完成的工作（即能线性表示出的所有向量）是完全相同的。它们生成了同一个向量空间。

### 2. 判别

判断两个向量组是否等价，主要有以下几种方法：

#### 方法一：根据定义直接判断（理论上可行，但实际操作复杂）

1. **判断 $S_1$ 能否被 $S_2$ 线性表示**：
    - 对于 $S_1$ 中的每一个向量 $\mathbf{\alpha}_i$，检查它是否能被 $S_2$ 线性表示。
    - 这需要解 $m$ 个线性方程组。例如，对于 $\mathbf{\alpha}_1$，解 $k_1 \mathbf{\beta}_1 + \cdots + k_s \mathbf{\beta}_s = \mathbf{\alpha}_1$。

2. **判断 $S_2$ 能否被 $S_1$ 线性表示**：
    - 对于 $S_2$ 中的每一个向量 $\mathbf{\beta}_j$，检查它是否能被 $S_1$ 线性表示。
    - 这需要解 $s$ 个线性方程组。例如，对于 $\mathbf{\beta}_1$，解 $c_1 \mathbf{\alpha}_1 + \cdots + c_m \mathbf{\alpha}_m = \mathbf{\beta}_1$。

如果这两个条件都满足，则 $S_1 \simeq S_2$。这种方法计算量大，不常用。

#### 方法二：利用秩的性质（最常用且高效的方法）

**定理**：向量组 $S_1$ 与向量组 $S_2$ 等价的充要条件是：
1. $S_1$ 中的每一个向量都可以被 $S_2$ 线性表示。
2. $S_2$ 中的每一个向量都可以被 $S_1$ 线性表示。
**等价于**：
**$rank(S_1) = rank(S_2) = rank(S_1 \cup S_2)$**

其中，$S_1 \cup S_2$ 表示将两个向量组的所有向量合并在一起形成的新向量组。

**具体步骤**：

1. **计算 $rank(S_1)$**：将 $S_1$ 中的向量作为列向量（或行向量）构成矩阵 $A_1$，求 $rank(A_1)$。
2. **计算 $rank(S_2)$**：将 $S_2$ 中的向量作为列向量（或行向量）构成矩阵 $A_2$，求 $rank(A_2)$。
3. **计算 $rank(S_1 \cup S_2)$**：将 $S_1$ 和 $S_2$ 中的所有向量合并在一起，构成一个大矩阵 $A = (A_1 | A_2)$（如果都是列向量），求 $rank(A)$。
4. **比较秩**：如果 $rank(A_1) = rank(A_2) = rank(A)$，则向量组 $S_1$ 与 $S_2$ 等价。

**示例**：判断向量组 $S_1 = \{\mathbf{\alpha}_1=(1,0,1)^T, \mathbf{\alpha}_2=(0,1,1)^T\}$ 与 $S_2 = \{\mathbf{\beta}_1=(1,1,2)^T, \mathbf{\beta}_2=(1,-1,0)^T\}$ 是否等价。

1. **计算 $rank(S_1)$**：
    $$ A_1 = \begin{pmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{pmatrix} $$
    显然，$\mathbf{\alpha}_1$ 和 $\mathbf{\alpha}_2$ 线性无关（不成比例），所以 $rank(A_1) = 2$。

2. **计算 $rank(S_2)$**：
    $$ A_2 = \begin{pmatrix} 1 & 1 \\ 1 & -1 \\ 2 & 0 \end{pmatrix} $$
    显然，$\mathbf{\beta}_1$ 和 $\mathbf{\beta}_2$ 线性无关（不成比例），所以 $rank(A_2) = 2$。

3. **计算 $rank(S_1 \cup S_2)$**：
    将所有向量合并构成矩阵 $A = (A_1 | A_2)$：
    $$ A = \begin{pmatrix} 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & -1 \\ 1 & 1 & 2 & 0 \end{pmatrix} $$
    进行初等行变换：
    $$ \begin{pmatrix} 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & -1 \\ 1 & 1 & 2 & 0 \end{pmatrix} \xrightarrow{R_3 - R_1} \begin{pmatrix} 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & -1 \\ 0 & 1 & 1 & -1 \end{pmatrix} $$
    $$ \xrightarrow{R_3 - R_2} \begin{pmatrix} 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & -1 \\ 0 & 0 & 0 & 0 \end{pmatrix} $$
    行阶梯形矩阵有 2 个非零行，所以 $rank(A) = 2$。

4. **比较秩**：
    $rank(S_1) = 2$，$rank(S_2) = 2$，$rank(S_1 \cup S_2) = 2$。
    由于 $rank(S_1) = rank(S_2) = rank(S_1 \cup S_2)$，所以向量组 $S_1$ 与 $S_2$ 等价。

这个判别方法非常强大，因为它将抽象的"线性表示"问题转化为了具体的"矩阵秩"的计算，大大简化了操作。

## 03-4 向量空间 (Vector Space)

### 1. 概念

**向量空间**（或线性空间）是一个由向量组成的集合 $V$，并且定义了两种运算：**向量加法**和**标量乘法**。这些运算满足一系列特定的公理（比如加法交换律、结合律，乘法分配律等）。

在考研数学中，我们通常讨论的是**有限维向量空间**，比如 $R^n$（所有 $n$ 维实向量的集合）。

#### 1. 基 (Basis)

- **定义**：向量空间 $V$ 的一个子集 $B = \{\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n\}$ 如果满足以下两个条件，就称它为 $V$ 的一个**基**：
    1. $B$ 是**线性无关**的。
    2. $B$ 中的向量可以**线性表示** $V$ 中的**任何一个向量**（即 $B$ 是 $V$ 的一个生成集）。

- **重要性**：基是向量空间中"最精简"的线性无关向量组，它能"张成"整个空间。空间中的任何一个向量都可以被基唯一地线性表示。
- **标准基**：对于 $R^n$ 空间，最常见的基是标准基：
    $$ \mathbf{e}_1 = (1, 0, \ldots, 0)^T $$
    $$ \mathbf{e}_2 = (0, 1, \ldots, 0)^T $$
    $$ \vdots $$
    $$ \mathbf{e}_n = (0, 0, \ldots, 1)^T $$

#### 2. 坐标 (Coordinates)

- **定义**：如果 $B = \{\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n\}$ 是向量空间 $V$ 的一个基，那么对于 $V$ 中的任意一个向量 $\mathbf{\alpha}$，它都可以被基唯一地线性表示为：
    $$ \mathbf{\alpha} = x_1 \mathbf{e}_1 + x_2 \mathbf{e}_2 + \cdots + x_n \mathbf{e}_n $$      这组唯一的标量 $(x_1, x_2, \ldots, x_n)$ 就称为向量 $\mathbf{\alpha}$ 在基 $B$ 下的**坐标**。通常写成列向量形式 $[\mathbf{\alpha}]_B = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$。

- **重要性**：坐标将抽象的向量具体化为一组数字，使得我们可以通过数值计算来处理向量。

#### 3. 维数 (Dimension)

- **定义**：向量空间 $V$ 的**维数**是指它的任何一个基中所含向量的个数。
- **表示**：记作 $\dim(V)$。
- **重要性**：维数是向量空间的一个基本属性，它告诉我们这个空间有多少个"自由度"。例如，平面是二维的，空间是三维的。

### 2. 基变换与坐标变换

在同一个向量空间中，我们可以选择不同的基。当基发生变化时，同一个向量的坐标也会随之改变，这就是基变换和坐标变换。

#### 1. 变换公式

假设我们有两个基：
- 旧基：$B = \{\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_n\}$
- 新基：$B' = \{\mathbf{e}'_1, \mathbf{e}'_2, \ldots, \mathbf{e}'_n\}$

任何一个向量 $\mathbf{\alpha}$ 在旧基 $B$ 下的坐标为 $[\mathbf{\alpha}]_B = \mathbf{x}$，在新基 $B'$ 下的坐标为 $[\mathbf{\alpha}]_B' = \mathbf{x}'$。

我们希望找到一个关系，将 $\mathbf{x}$ 和 $\mathbf{x}'$ 联系起来。

#### 2. 过渡矩阵 (Transition Matrix)

- **定义**：将新基 $B'$ 中的每个向量用旧基 $B$ 线性表示：
    $$ \mathbf{e}'_j = p_{1j} \mathbf{e}_1 + p_{2j} \mathbf{e}_2 + \cdots + p_{nj} \mathbf{e}_n $$
    将这些系数作为列向量构成一个矩阵 $P = (p_{ij})$，这个矩阵 $P$ 就称为从**旧基 $B$ 到新基 $B'$ 的过渡矩阵**。
    $$ P = ([\mathbf{e}'_1]_B, [\mathbf{e}'_2]_B, \ldots, [\mathbf{e}'_n]_B) $$
    也就是说，过渡矩阵的列是新基向量在旧基下的坐标。

- **坐标变换公式**：
    有了过渡矩阵 $P$，同一个向量 $\mathbf{\alpha}$ 在新旧基下的坐标关系为：
    $$ \mathbf{x} = P \mathbf{x}' $$
    或者，如果想从旧坐标得到新坐标：
    $$ \mathbf{x}' = P^{-1} \mathbf{x} $$

- **直观理解**：
    - $\mathbf{x} = P \mathbf{x}'$ 意味着：旧坐标 = (旧基表示新基的矩阵) $\times$ 新坐标。
    - $\mathbf{x}' = P^{-1} \mathbf{x}$ 意味着：新坐标 = (新基表示旧基的矩阵) $\times$ 旧坐标。
- **重要性**：过渡矩阵是连接不同坐标系的桥梁。它在理解线性变换的矩阵表示、相似矩阵等概念时非常关键。

**示例**：在 $R^2$ 中，旧基 $B = \{\mathbf{e}_1=(1,0)^T, \mathbf{e}_2=(0,1)^T\}$，新基 $B' = \{\mathbf{e}'_1=(1,1)^T, \mathbf{e}'_2=(-1,1)^T\}$。

1. **求过渡矩阵 $P$**：
    将新基向量用旧基表示：
    $$ \mathbf{e}'_1 = 1 \cdot \mathbf{e}_1 + 1 \cdot \mathbf{e}_2 \Rightarrow [\mathbf{e}'_1]_B = \begin{pmatrix} 1 \\ 1 \end{pmatrix} $$
    $$ \mathbf{e}'_2 = -1 \cdot \mathbf{e}_1 + 1 \cdot \mathbf{e}_2 \Rightarrow [\mathbf{e}'_2]_B = \begin{pmatrix} -1 \\ 1 \end{pmatrix} $$
    所以，从旧基 $B$ 到新基 $B'$ 的过渡矩阵为：
    $$ P = \begin{pmatrix} 1 & -1 \\ 1 & 1 \end{pmatrix} $$

2. **坐标变换**：
    假设向量 $\mathbf{\alpha} = (2, 3)^T$。它在旧基 $B$ 下的坐标就是 $[\mathbf{\alpha}]_B = \begin{pmatrix} 2 \\ 3 \end{pmatrix}$。
    要求它在新基 $B'$ 下的坐标 $[\mathbf{\alpha}]_B' = \mathbf{x}'$。
    我们使用公式 $\mathbf{x}' = P^{-1} \mathbf{x}$。
    首先求 $P^{-1}$：
    $$ P^{-1} = \frac{1}{\det(P)} \text{adj}(P) = \frac{1}{1 \cdot 1 - (-1) \cdot 1} \begin{pmatrix} 1 & 1 \\ -1 & 1 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 & 1 \\ -1 & 1 \end{pmatrix} $$
    然后计算 $\mathbf{x}'$:
    $$ \mathbf{x}' = \frac{1}{2} \begin{pmatrix} 1 & 1 \\ -1 & 1 \end{pmatrix} \begin{pmatrix} 2 \\ 3 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 1 \cdot 2 + 1 \cdot 3 \\ -1 \cdot 2 + 1 \cdot 3 \end{pmatrix} = \frac{1}{2} \begin{pmatrix} 5 \\ 1 \end{pmatrix} = \begin{pmatrix} 5/2 \\ 1/2 \end{pmatrix} $$
    所以，向量 $\mathbf{\alpha}$ 在新基 $B'$ 下的坐标是 $(5/2, 1/2)^T$。
    验证：$5/2 \cdot \mathbf{e}'_1 + 1/2 \cdot \mathbf{e}'_2 = 5/2 \begin{pmatrix} 1 \\ 1 \end{pmatrix} + 1/2 \begin{pmatrix} -1 \\ 1 \end{pmatrix} = \begin{pmatrix} 5/2 - 1/2 \\ 5/2 + 1/2 \end{pmatrix} = \begin{pmatrix} 4/2 \\ 6/2 \end{pmatrix} = \begin{pmatrix} 2 \\ 3 \end{pmatrix}$，正确！
